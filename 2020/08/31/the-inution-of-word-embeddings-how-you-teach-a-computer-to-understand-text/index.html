<style>
.sidebar-button-desc {
  color: darkgrey !important;
}
.sidebar-button-icon {
  color: darkgrey !important;
}
.sidebar-profile-name {
  color: darkgrey !important;
</style><!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.74.3 with theme Tranquilpeak 0.4.8-BETA">
<meta name="author" content="Heike W">
<meta name="keywords" content="Word Embeddings, One-Hot-Encoding, Word2Vec, BERT, SQL, Data Science, Analytics, ElasticSeacg, Python, Pandas, Analytics, R Markdown, Research, phd, Text Mining, Natural Language Processing, Hive, Data Analysis, Data Mining">
<meta name="description" content="Humans intuitively understand the meaning of words: Which words are similar, opposites or related to each other? But our machine learning models do not have this intuition. Word embeddings are numeric vectors that represent text. These vectors are learned through neural networks. The objective when creating these embedding vectors is to capture as much &ldquo;meaning&rdquo; as possible: Related words should be closer together than unrelated words. Also, they should be able to preserve mathematical relationships between words such as">


<meta property="og:description" content="Humans intuitively understand the meaning of words: Which words are similar, opposites or related to each other? But our machine learning models do not have this intuition. Word embeddings are numeric vectors that represent text. These vectors are learned through neural networks. The objective when creating these embedding vectors is to capture as much &ldquo;meaning&rdquo; as possible: Related words should be closer together than unrelated words. Also, they should be able to preserve mathematical relationships between words such as">
<meta property="og:type" content="article">
<meta property="og:title" content="The Inution of Word Embeddings: How you Teach A Computer to Understand Text">
<meta name="twitter:title" content="The Inution of Word Embeddings: How you Teach A Computer to Understand Text">
<meta property="og:url" content="/2020/08/31/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text/">
<meta property="twitter:url" content="/2020/08/31/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text/">
<meta property="og:site_name" content="My journey from Data Analyst to Data Science">
<meta property="og:description" content="Humans intuitively understand the meaning of words: Which words are similar, opposites or related to each other? But our machine learning models do not have this intuition. Word embeddings are numeric vectors that represent text. These vectors are learned through neural networks. The objective when creating these embedding vectors is to capture as much &ldquo;meaning&rdquo; as possible: Related words should be closer together than unrelated words. Also, they should be able to preserve mathematical relationships between words such as">
<meta name="twitter:description" content="Humans intuitively understand the meaning of words: Which words are similar, opposites or related to each other? But our machine learning models do not have this intuition. Word embeddings are numeric vectors that represent text. These vectors are learned through neural networks. The objective when creating these embedding vectors is to capture as much &ldquo;meaning&rdquo; as possible: Related words should be closer together than unrelated words. Also, they should be able to preserve mathematical relationships between words such as">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2020-08-31T21:17:58">
  
  
    <meta property="article:modified_time" content="2020-08-31T21:17:58">
  
  
  
    
      <meta property="article:section" content="Machine Learning">
    
      <meta property="article:section" content="NLP">
    
  
  
    
      <meta property="article:tag" content="Word Embeddings">
    
      <meta property="article:tag" content="One-Hot-Encoding">
    
      <meta property="article:tag" content="Word2Vec">
    
      <meta property="article:tag" content="BERT">
    
  


<meta name="twitter:card" content="summary">







  <meta property="og:image" content="/post/net.jpg">
  <meta property="twitter:image" content="/post/net.jpg">




    <title>The Inution of Word Embeddings: How you Teach A Computer to Understand Text</title>

    <link rel="icon" href="/favicon.png">
    

    

    <link rel="canonical" href="/2020/08/31/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css" />
    
    

    
      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-128956833-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="1">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="/">My journey from Data Analyst to Data Science</a>
  </div>
  
    
      <a class="header-right-icon open-algolia-search"
         href="/#search">
    
    
      <i class="fa fa-lg fa-search"></i>
    
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="1">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="/#about">
          <img class="sidebar-profile-picture" src="/me.jpg" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Heike W</h4>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link open-algolia-search" href="/#search">
    
      <i class="sidebar-button-icon fa fa-lg fa-search"></i>
      
      <span class="sidebar-button-desc">Search</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      
  <div class="post-header-cover
              text-left
              "
       style="background-image:url('/post/net.jpg')"
       data-behavior="1">
    
      <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      The Inution of Word Embeddings: How you Teach A Computer to Understand Text
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-08-31T21:17:58&#43;02:00">
        
  
  
  
  
    31 August 2020
  

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="/categories/machine-learning">Machine Learning</a>, 
    
      <a class="category-link" href="/categories/nlp">NLP</a>
    
  

  </div>

</div>
    
  </div>


      <div id="main" data-behavior="1"
        class="hasCover
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>Humans intuitively understand the meaning of words: Which words are similar, opposites or related to each other? But our machine learning models do not have this intuition. Word embeddings are numeric vectors that represent text. These vectors are learned through neural networks. The objective when creating these embedding vectors is to capture as much &ldquo;meaning&rdquo; as possible: Related words should be closer together than unrelated words. Also, they should be able to preserve mathematical relationships between words such as</p>
<p>king — man = queen — woman</p>
<p>france - paris = italy - rome.</p>
<p>In this blogpost, we will start with (probably) the simplest way to numerically represent text: One-hot-encoding. Then we will look into two famous word embedding models: Word2Vec and BERT and learn which kind of relationships between words are captured.</p>
<h2 id="whats-wrong-with-just-using-one-hot-encoding-aka-dummy-variables">What&rsquo;s wrong with just using One-Hot-Encoding (a.k.a. Dummy variables)?</h2>
<p>The easiest way to encode words is through one One-Hot encoded vectors: Each unique word or token has a unique dimension and will be represented with a 1 if present and with 0s everywhere else. Your result is a huge and sparse matrix with 0s and 1s. Let&rsquo;s take two sentences</p>
<ul>
<li>s1: &ldquo;I am sitting on the river bank.&rdquo;</li>
<li>s2: &ldquo;I am going to the bank to get some money&rdquo;.</li>
</ul>
<p>The two sentences share some common words (&ldquo;I&rdquo;, &ldquo;am&rdquo;, &ldquo;the&rdquo; and &ldquo;bank&rdquo;). The word &ldquo;bank&rdquo;, however, has a different meaning in both sentences. If you use One-Hot-Encoding you get the following matrix:</p>
<table>
<thead>
<tr>
<th></th>
<th>I</th>
<th>am</th>
<th>going</th>
<th>to</th>
<th>the</th>
<th>river</th>
<th>bank</th>
<th>get</th>
<th>some</th>
<th>money</th>
</tr>
</thead>
<tbody>
<tr>
<td>s1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>s2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>The larger the text, the larger the matrix. If you applied One-Hot-Encoding to fulltext data you would get larger, sparser matrices. So One-Hot-Encoding may make sense, if you have a limited number of words in your text (e.g. if you want to vectorize gender or a profession from a text field).</p>
<p>In addition, it disregards how many times a word appears in a sentence and therefore does not capture the importance of words. Every word will always get a value between 0 or 1. For considering the importance of words, a TF-IDF-Measure transformation can be applied. By weighting words with their term frequency divided by their inverse document frequency, rare words that appear a lot in one sentence are weighted more than words that appear a lot in every sentence and in many sentences (e. g. stopwords such as &ldquo;the&rdquo;, &ldquo;a&rdquo;, &ldquo;or&rdquo;).</p>
<p>But there would still be two major drawbacks: You cannot differentiate between a river bank and the bank, where you store your money. And you cannot grasp the relationship between words: Are words similar? Are they somehow related?</p>
<h2 id="word2vec">Word2Vec</h2>
<p>Word2vec is a three-layer neural network in which the first is the input layer and the last layers are the output layer. Input and output layers are one-hot-encoded vectors of the text. The middle layer builds a latent representation, so the input words are transformed into the output vector representation.</p>
<ul>
<li><strong>inputs represented as one-hot vectors</strong></li>
<li><strong>outputs represented as one hot vectors</strong> (in training)</li>
<li><strong>embedding layer</strong>: matrix of all the words found in the vocabulary and their embedding (some numeric value that isinitialized randomly)[vocab size x embedding size]</li>
</ul>
<p>Google has trained the Word2Vec model on Google News Data and made it available <a href="(https://code.google.com/archive/p/word2vec/)">for download</a>. There are two approaches:</p>
<p><strong>Continuous Bag of Words (CBOW)</strong> predicts a word given its context. In other words: it learns an embedding by predicting a word by using the surrounding words it as features.</p>
<p><strong>Continous Skip Gram</strong> predicts the context having one target word as the input. It learns an embedding by predicting the context by using the words as features.</p>
<p><img src="/post/Cbow_vs_scipGram.jpg" alt="models"></p>
<p>If you want to retrain or train a Word2Vec Model, you have to define a context, the number of words appearing on the left and right of a focus word. If you choose for example 3, then the context would include the 3 words before and the 3 words after the focus word. Note that this approach does not consider the order of words. Either a word is in the context or it is not. Word2Vec then uses a stochastic optimizer to train the neural network.</p>
<p>The creators evaluated the model by marking sure similar words have similar vectors. You can visualize similarity by analyzing cosine similarity or by plotting the embeddings from the original models, e.g. <a href="https://projector.tensorflow.org">https://projector.tensorflow.org</a>.</p>
<p>Word2Vec registers relations among words. So if you subtract the man from king you get the same as when you subtract the women from queen. Great, this model is able to retain mathematical relationships between words.
<img src="/post/embedding_vectors.jpg" alt="w2v"></p>
<p>If you check for the five most similar words in terms of cosine similarity for &ldquo;bank&rdquo;, you get &ldquo;banks&rdquo;, &ldquo;monetary&rdquo;, &ldquo;banking&rdquo;, &ldquo;financial&rdquo; and &ldquo;trade&rdquo;. So the model seems to primarily interpret &ldquo;bank&rdquo; in the sense of a financial institute. Don&rsquo;t forget the model is trained on news data, so if you apply it to very specific contexts (e.g. scientific domain data), you may lack specific words and relations. The word &ldquo;bank&rdquo; will always have the same numeric representation. So the model fails to understand the difference between the river bank and the financial institute bank.<br>
If you wanted to use Word2Vec on text data about river banks, you would have to retrain the model.</p>
<h2 id="bert">BERT</h2>
<p>One embedding model, that became pretty popular and that was considered as a great break-through, is BERT. BERT produces word representations that are dynamically informed by the words around them. That means: BERT understands that the word &ldquo;bank&rdquo; can mean something different depending on the context.</p>
<p>So what is exactly BERT? BERT stands for Bidirectional Encoder Representations from Transformers. Let&rsquo;s break that up: Bidirectional means, that instead of going sequentially through the words, it goes from left to right and from right to left. It uses the transformer logic (a special type of neural net that contains two sublayers and a fully connected feed forward network) to encode representations of words. There are two English versions of BERT: A smaller base version and a large Version. The larger version has double the amount of encoders and attention heads and more than 3 times as many parameters</p>
<table>
<thead>
<tr>
<th></th>
<th align="left">nr Encoders</th>
<th align="left">nr biderectional self-attention heads</th>
<th align="left">nr parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>BERT base</td>
<td align="left">12</td>
<td align="left">12</td>
<td align="left">110</td>
</tr>
<tr>
<td>BERT large</td>
<td align="left">24</td>
<td align="left">24</td>
<td align="left">340</td>
</tr>
</tbody>
</table>
<p>BERT follows a 2-step process: pre-training and fine-tuning</p>
<ol>
<li>
<p>Pre-Train a language model on a large unlabelled text corpus (data extracted from BooksCorpus (800M words) and English Wikipedia (2,500M words)) by using a masked language model and next sentence prediction. This type of training is novel as it is an unsupervised approach that can be easily transferred to other datasets without manually labeling data.</p>
</li>
<li>
<p>Fine-tune the model. You can finetune the model with a layer of untrained neurons as a feedforward layer on top of the pre-trained BERT for specific domain knowledge (But you could also add a supervised additional output layer to solve for example classification tasks or build chatbots, which makes the model really powerful)</p>
</li>
</ol>
<h2 id="summary">Summary</h2>
<p>We have seen three approaches on how to create numeric representations of text. Let&rsquo;s briefly recap, what the different approaches are helpful for:</p>
<table>
<thead>
<tr>
<th></th>
<th>One-Hot-Encoding</th>
<th>Word2Vec</th>
<th>BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td>relationships between words</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>consider the context of a word</td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
</tbody>
</table>
<p>There exists many more and there are also numerous extensions for Word2Vec and BERT.</p>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="/tags/word-embeddings/">Word Embeddings</a>

  <a class="tag tag--primary tag--small" href="/tags/one-hot-encoding/">One-Hot-Encoding</a>

  <a class="tag tag--primary tag--small" href="/tags/word2vec/">Word2Vec</a>

  <a class="tag tag--primary tag--small" href="/tags/bert/">BERT</a>

                  </div>
                
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2020/07/12/jupyter-notebooks-boost-your-productivity-with-extensions-and-magic-commands/" data-tooltip="Jupyter Notebooks: Boost your productivity with Extensions and Magic Commands">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=/2020/08/31/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=/2020/08/31/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
                <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 Heike W. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="1">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2020/07/12/jupyter-notebooks-boost-your-productivity-with-extensions-and-magic-commands/" data-tooltip="Jupyter Notebooks: Boost your productivity with Extensions and Magic Commands">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=/2020/08/31/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=/2020/08/31/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="1">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=%2F2020%2F08%2F31%2Fthe-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text%2F">
          <i class="fa fa-facebook-official"></i><span>Share on Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=%2F2020%2F08%2F31%2Fthe-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text%2F">
          <i class="fa fa-twitter"></i><span>Share on Twitter</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="/me.jpg" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Heike W</h4>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Data Scientist
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Germany
      </div>
    
  </div>
</div>

    

    
  
    
      <div id="cover" style="background-image:url('source/assets/img/banner');"></div>
    
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = '\/2020\/08\/31\/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text\/';
          
            this.page.identifier = '\/2020\/08\/31\/the-inution-of-word-embeddings-how-you-teach-a-computer-to-understand-text\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'Heike';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  




    
  </body>
</html>

