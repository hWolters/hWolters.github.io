<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Word2Vec on My journey from Data Analyst to Data Science</title>
    <link>/tags/word2vec/</link>
    <description>Recent content in Word2Vec on My journey from Data Analyst to Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Aug 2020 21:17:58 +0200</lastBuildDate>
    
	<atom:link href="/tags/word2vec/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Natural Language Processing: The Intuition of Word Embeddings</title>
      <link>/2020/08/25/natural-language-processing-the-intuition-of-word-embeddings/</link>
      <pubDate>Tue, 25 Aug 2020 21:17:58 +0200</pubDate>
      
      <guid>/2020/08/25/natural-language-processing-the-intuition-of-word-embeddings/</guid>
      <description>Humas intuitively understand the meaning of words: Which words are similar, opposites or related to each other? But our machine learning models do not have this intuition. Word embeddings are numeric vectors that represent text. These vectors are learned through neural networks. The objective when creating these embedding vectors is to captures as much &amp;ldquo;meaning&amp;rdquo; as possible: Related words should be closer together than unrelated words. Also, they should be able to perserve mathematical relationships between words such as</description>
    </item>
    
  </channel>
</rss>