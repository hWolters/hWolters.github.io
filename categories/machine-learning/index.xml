<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on A Potpourri of Data Science &amp; Data Engineering Topics</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on A Potpourri of Data Science &amp; Data Engineering Topics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Apr 2021 15:15:38 +0200</lastBuildDate><atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Biases in learning to rank models and three approaches to deal with them</title>
      <link>/2021/04/29/biases-in-learning-to-rank-models-and-three-approaches-to-deal-with-them/</link>
      <pubDate>Thu, 29 Apr 2021 15:15:38 +0200</pubDate>
      
      <guid>/2021/04/29/biases-in-learning-to-rank-models-and-three-approaches-to-deal-with-them/</guid>
      <description>&lt;p&gt;Search engines rely on models, which rank the matching results for a given user query. These models optimize the order of items. They learn how to rank items in a result list, therefore the name Learning-to-Rank (LTR) models.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pointwise, Pairswise and Listwise Learning to Rank Models - Three Approaches to Optimize Relative Ordering</title>
      <link>/2020/10/15/pointwise-pairswise-and-listwise-learning-to-rank-models-three-approaches-to-optimize-relative-ordering/</link>
      <pubDate>Thu, 15 Oct 2020 18:36:40 +0100</pubDate>
      
      <guid>/2020/10/15/pointwise-pairswise-and-listwise-learning-to-rank-models-three-approaches-to-optimize-relative-ordering/</guid>
      <description>&lt;p&gt;In many scenarios, such as a google search or a product recommendation in an online shop, we have tons of data and limited space to display it. We cannot show all the products of an online shop to the user as a possible next best offer. Neither would a user want to scroll through all the pages indexed by a search engine to find the most relevant page that matches his search keywords. The most relevant content should be on top. Learning to rank (LTR) models are supervised machine learning models that attempt to optimize the order of items. So compared to classification or regression models, they do not care about exact scores or predictions, but the relative order. LTR models are typically applied in search engines, but gained popularity in other fields such as product recommendations as well.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AI-Machine-Learning-Buzzword-Bingo</title>
      <link>/2020/09/10/ai-machine-learning-buzzword-bingo/</link>
      <pubDate>Thu, 10 Sep 2020 22:20:39 +0200</pubDate>
      
      <guid>/2020/09/10/ai-machine-learning-buzzword-bingo/</guid>
      <description>I was recently invited to join a panel discussion among developers to dispel the myth of the typical BS Buzzword Bingo around machine learning and AI. In this blog post, I will share some buzzwords we talked about with a little description and links. Ooops, I already used some buzzwords. So let&amp;rsquo;s start.
AI (Artificial Intelligence) is the magic portion to fix all problems of all companies and will make us unemployed in the future.</description>
    </item>
    
    <item>
      <title>The Intuition of Word Embeddings: How you Teach A Computer to Understand Text</title>
      <link>/2020/08/31/the-intuition-of-word-embeddings-how-you-teach-a-computer-to-understand-text/</link>
      <pubDate>Mon, 31 Aug 2020 21:17:58 +0200</pubDate>
      
      <guid>/2020/08/31/the-intuition-of-word-embeddings-how-you-teach-a-computer-to-understand-text/</guid>
      <description>Humans intuitively understand the meaning of words: Which words are similar, opposites or related to each other? But our machine learning models do not have this intuition. Word embeddings are numeric vectors that represent text. These vectors are learned through neural networks. The objective when creating these embedding vectors is to capture as much &amp;ldquo;meaning&amp;rdquo; as possible: Related words should be closer together than unrelated words. Also, they should be able to preserve mathematical relationships between words such as</description>
    </item>
    
  </channel>
</rss>
